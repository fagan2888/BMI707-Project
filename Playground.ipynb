{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DiGenScrGhv"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mwestt/BMI707-Project/blob/master/Playground.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "Use this notebook to import functions and play around with specific model architectures. If you're running this in colab, we'll need to clone the repo this notebook is in, as well as the repo with the data in. \n",
    "\n",
    "**Make sure you comment out this next cell if running locally!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "aK7lBuMJsm-e",
    "outputId": "115c790c-58ab-425b-dc8a-0a915e06d22c"
   },
   "outputs": [],
   "source": [
    "# Clone our project repo so we've got our code\n",
    "!git clone https://github.com/mwestt/BMI707-Project.git\n",
    "\n",
    "# Clone covid-chestxray-dataset repo for data and metadata\n",
    "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git\n",
    "\n",
    "# Move data and metadata to project repo and cd to it\n",
    "!mv covid-chestxray-dataset/images/ BMI707-Project/\n",
    "!mv covid-chestxray-dataset/metadata.csv BMI707-Project/\n",
    "%cd BMI707-Project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1CZ7qD_rax9"
   },
   "source": [
    "## Benchmark Classifier - No Augmentation\n",
    "\n",
    "First, we'll need to import the necessary functions from the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "P_h3SqmlrGhw",
    "outputId": "5422c205-91f9-4168-a4c6-b2230f255976"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientid</th>\n",
       "      <th>offset</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>finding</th>\n",
       "      <th>survival</th>\n",
       "      <th>intubated</th>\n",
       "      <th>intubation_present</th>\n",
       "      <th>went_icu</th>\n",
       "      <th>needed_supplemental_O2</th>\n",
       "      <th>extubated</th>\n",
       "      <th>temperature</th>\n",
       "      <th>pO2_saturation</th>\n",
       "      <th>leukocyte_count</th>\n",
       "      <th>neutrophil_count</th>\n",
       "      <th>lymphocyte_count</th>\n",
       "      <th>view</th>\n",
       "      <th>modality</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>folder</th>\n",
       "      <th>filename</th>\n",
       "      <th>doi</th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>clinical_notes</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "      <td>74.0</td>\n",
       "      <td>SARS</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>2004</td>\n",
       "      <td>Mount Sinai Hospital, Toronto, Ontario, Canada</td>\n",
       "      <td>images</td>\n",
       "      <td>SARS-10.1148rg.242035193-g04mr34g0-Fig8a-day0....</td>\n",
       "      <td>10.1148/rg.242035193</td>\n",
       "      <td>https://pubs.rsna.org/doi/10.1148/rg.242035193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SARS in a 74-year-old man who developed sympto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>72.0</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hospital Universitario Doctor Peset, Valencia,...</td>\n",
       "      <td>images</td>\n",
       "      <td>16660_3_1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.eurorad.org/case/16660</td>\n",
       "      <td>CC BY-NC-SA 4.0</td>\n",
       "      <td>A 72-year-old woman admitted with acute respir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>60.0</td>\n",
       "      <td>COVID-19, ARDS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>2020</td>\n",
       "      <td>Spain</td>\n",
       "      <td>images</td>\n",
       "      <td>covid-19-pneumonia-rapidly-progressive-admissi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://radiopaedia.org/cases/covid-19-pneumon...</td>\n",
       "      <td>CC BY-NC-SA</td>\n",
       "      <td>Fever and odynophagia. Trip to Italy 7 days ag...</td>\n",
       "      <td>Case courtesy of Dr Edgar Lorente, Radiopaedia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>154</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images</td>\n",
       "      <td>radiol.2020201160.fig2d.jpeg</td>\n",
       "      <td>10.1148/radiol.2020201160</td>\n",
       "      <td>https://pubs.rsna.org/doi/full/10.1148/radiol....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>M</td>\n",
       "      <td>47.0</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>X-ray</td>\n",
       "      <td>March 4, 2020</td>\n",
       "      <td>Italy</td>\n",
       "      <td>images</td>\n",
       "      <td>F4341CE7-73C9-45C6-99C8-8567A5484B63.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.sirm.org/2020/03/10/covid-19-caso-34/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male patient, 47 years old. Remote history cha...</td>\n",
       "      <td>Credit to G.Patelli , F.Besana , S. Paganoni *...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     patientid  offset  ...                                        other_notes  Unnamed: 27\n",
       "9            3     4.0  ...                                                NaN          NaN\n",
       "316        178     1.0  ...                                                NaN          NaN\n",
       "183         96     0.0  ...  Case courtesy of Dr Edgar Lorente, Radiopaedia...          NaN\n",
       "273        154    10.0  ...                                                NaN          NaN\n",
       "101         51     3.0  ...  Credit to G.Patelli , F.Besana , S. Paganoni *...          NaN\n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_data import load_metadata, load_data\n",
    "from benchmark_classifier import train_benchmark\n",
    "\n",
    "# Load the metadata csv\n",
    "df_train, df_val = load_metadata('metadata.csv')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL5d6i5ErGh3"
   },
   "outputs": [],
   "source": [
    "# Load training and validation images from metadata csv's\n",
    "images_train, labels_train = load_data(df_train)\n",
    "images_val, labels_val = load_data(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wogSP5hrGh7"
   },
   "source": [
    "Here we'll define the Conv Net with the function `playground_model()`. Toy around with some of the parameters and see if you can get a decent-looking AUC (I'd say we're aiming for **0.7** but maybe we can do better). In general we want as simple a model as possible to be able to get perfect training accuracy, and see how good we can get our validation AUC. Some things to try:\n",
    "\n",
    "Mainly: \n",
    "- **Add extra or remove existing `Conv2D` layers.**\n",
    "- **Change number of filters in each `Conv2D` layer (first argument)**\n",
    "\n",
    "*But also:*\n",
    "- Add or remove `Dropout` layers (these layers are probably unnecessary here)\n",
    "- Change max pooling layers to average pooling\n",
    "- Smaller `Dense` layer in the final layer\n",
    "- Global average pooling instead of the final `Dense` layer\n",
    "\n",
    "**Make sure you're using a GPU Runtime!** Go to *Runtime > Change runtime type > Hardware accelerator > GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "id": "tD2NtlphrGh8",
    "outputId": "18a1aa7a-1a67-4143-f356-7e147870448c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\i_can\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\i_can\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\i_can\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 189 samples, validate on 94 samples\n",
      "Epoch 1/12\n",
      "160/189 [========================>.....] - ETA: 1s - loss: 10.3390 - accuracy: 0.7125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-14a54ec13392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Train benchmark model - if you find good parameters, set `save=True`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m trained_model = train_benchmark(model, images_train, images_val, labels_train, labels_val, \n\u001b[1;32m---> 41\u001b[1;33m                                 epochs=12, batch_size=32, save=False)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Evaluation metrics - pay attention to AUC!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\CBQG Courses\\Spring 2020\\BMI707\\Project\\BMI707-Project\\benchmark_classifier.py\u001b[0m in \u001b[0;36mtrain_benchmark\u001b[1;34m(model, X_train, X_val, y_train, y_val, epochs, batch_size, save)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, \n\u001b[1;32m---> 84\u001b[1;33m               batch_size=batch_size, verbose=1)\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# Save model with timestamp as name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def playground_model():\n",
    "    \"\"\"Create Keras model using Sequential API.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Keras Sequential object\n",
    "        Keras Sequential model following the specified architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(256, 256, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Assign the model defined above\n",
    "model = playground_model()\n",
    "\n",
    "# Train benchmark model - if you find good parameters, set `save=True`\n",
    "trained_model = train_benchmark(model, images_train, images_val, labels_train, labels_val, \n",
    "                                epochs=12, batch_size=32, save=False)\n",
    "\n",
    "# Evaluation metrics - pay attention to AUC!\n",
    "print('Validation Labels:')\n",
    "print(labels_val)\n",
    "\n",
    "print('Predicted Labels:')\n",
    "y_pred = trained_model.predict_classes(images_val, verbose=1).T[0]\n",
    "print(y_pred)\n",
    "\n",
    "print('Predicted Probabilities')\n",
    "y_probs = trained_model.predict(images_val, verbose=1).T[0]\n",
    "print(y_probs)\n",
    "\n",
    "print('Prediction AUC')\n",
    "print(roc_auc_score(labels_val, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEjw_nrBrGh_"
   },
   "source": [
    "## Loading Model Workflow\n",
    "\n",
    "Feel free to add any code cells following this, for example loading your saved models and then using them to make predictions. For example, here's one I made earlier! Just save your own model with `save=True` in `train_benchmark()` above,\n",
    "and replace the filepath below with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "id": "CPr5XBBFrGiB",
    "outputId": "84808c86-62db-49aa-e922-bd8f28b37aed"
   },
   "outputs": [],
   "source": [
    "# # Load saved model and print summary information\n",
    "# loaded_model = load_model('model_bench_2020_04_22_22_58_23.h5')    \n",
    "# print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoT7hVWkrGiF"
   },
   "outputs": [],
   "source": [
    "# print('Validation Labels:')\n",
    "# print(labels_val)\n",
    "\n",
    "# print('Predicted Labels:')\n",
    "# y_pred = loaded_model.predict_classes(images_val, verbose=1).T[0]\n",
    "# print(y_pred)\n",
    "\n",
    "# print('Predicted Probabilities')\n",
    "# y_probs = loaded_model.predict(images_val, verbose=1).T[0]\n",
    "# # print(y_probs)\n",
    "\n",
    "# print('Prediction AUC')\n",
    "# print(roc_auc_score(labels_val, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Data Augmentation\n",
    "\n",
    "The following cell is much like the previous section, but we will now train on augmented data using Keras data augmentation, rather than on the images directly. There are a number of augmentation parameters to explore, take a look at the Keras documentation for the [`ImageDataGenerator` class](https://keras.io/preprocessing/image/) for potential arguments to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 189 validated image filenames.\n",
      "Found 94 validated image filenames.\n",
      "Epoch 1/12\n",
      "6/6 [==============================] - 18s 3s/step - loss: 6.1225 - accuracy: 0.6667 - val_loss: 0.6288 - val_accuracy: 0.7484\n",
      "Epoch 2/12\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.7461 - accuracy: 0.6875 - val_loss: 0.5360 - val_accuracy: 0.8376\n",
      "Epoch 3/12\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.4755 - accuracy: 0.8065 - val_loss: 0.4837 - val_accuracy: 0.8301\n",
      "Epoch 4/12\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.5331 - accuracy: 0.7708 - val_loss: 0.4137 - val_accuracy: 0.8344\n",
      "Epoch 5/12\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3740 - accuracy: 0.8387 - val_loss: 0.7636 - val_accuracy: 0.8248\n",
      "Epoch 6/12\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.7049 - accuracy: 0.7604 - val_loss: 0.5934 - val_accuracy: 0.8077\n",
      "Epoch 7/12\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4818 - accuracy: 0.8172 - val_loss: 0.6182 - val_accuracy: 0.8280\n",
      "Epoch 8/12\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3528 - accuracy: 0.8438 - val_loss: 0.4946 - val_accuracy: 0.8248\n",
      "Epoch 9/12\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.4344 - accuracy: 0.7812 - val_loss: 0.5010 - val_accuracy: 0.8013\n",
      "Epoch 10/12\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3006 - accuracy: 0.8817 - val_loss: 0.6615 - val_accuracy: 0.8057\n",
      "Epoch 11/12\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3076 - accuracy: 0.8854 - val_loss: 0.3829 - val_accuracy: 0.8153\n",
      "Epoch 12/12\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.2509 - accuracy: 0.8817 - val_loss: 0.4870 - val_accuracy: 0.8077\n"
     ]
    }
   ],
   "source": [
    "from load_data import data_generator_from_dataframe, ValidImageDataGenerator\n",
    "from benchmark_classifier import train_augmented_benchmark\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define ImageDataGenerator for training - tweak these arguments\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True,\n",
    "                                         width_shift_range=1)\n",
    "\n",
    "train_generator = data_generator_from_dataframe(train_datagen, df_train,\n",
    "                                                image_size=(256, 256), batch_size=16)\n",
    "\n",
    "# Simple generator for validation\n",
    "validation_generator = data_generator_from_dataframe(ValidImageDataGenerator(), df_val)\n",
    "\n",
    "# Reinstantiate model\n",
    "model = playground_model()\n",
    "\n",
    "# Train model with augmented data\n",
    "trained_model_aug = train_augmented_benchmark(model, train_generator, validation_generator,\n",
    "                                          epochs=12, steps_per_epoch=6, validation_steps=10, \n",
    "                                          save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Labels:\n",
      "[0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predicted Labels:\n",
      "94/94 [==============================] - 1s 13ms/step\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predicted Probabilities\n",
      "94/94 [==============================] - 1s 11ms/step\n",
      "[0.986323   0.776845   0.9189477  0.9927617  0.96600676 0.7223801\n",
      " 0.9929855  0.94768214 0.9185946  0.9950176  0.8442918  0.71621764\n",
      " 0.8862897  0.9164943  0.9870961  0.95292217 0.59593713 0.9930233\n",
      " 0.9618919  0.97825754 0.9205772  0.9999999  0.9974457  0.9327251\n",
      " 0.95959413 0.8863493  0.9438021  0.83269775 0.97989154 0.8145764\n",
      " 0.8910817  0.97636956 0.94992113 0.86872077 0.80468774 0.9988829\n",
      " 0.9713118  0.89961123 0.82265043 0.90378153 0.85487694 0.9791869\n",
      " 0.9061697  0.7777758  0.89426786 0.87539923 0.9327805  0.8140588\n",
      " 0.9949287  0.993062   0.85917795 0.9644083  0.9261545  0.973933\n",
      " 0.8836105  0.62067336 0.81856835 0.9992113  0.946298   0.9395849\n",
      " 0.90473145 0.9295189  0.76492405 0.9021504  0.9692236  0.85493135\n",
      " 0.94263345 0.9917697  0.9936584  0.9228803  0.8577881  0.8863588\n",
      " 0.4374328  0.76482046 0.56430596 0.9999562  0.9678083  0.7321767\n",
      " 0.94075084 0.87488663 0.863382   0.9989097  0.90808254 0.79322445\n",
      " 0.99674606 0.7901656  0.88350046 0.8212292  0.8571547  0.99526525\n",
      " 0.8915994  0.9893626  0.93024045 0.99265903]\n",
      "Prediction AUC\n",
      "0.5232371794871795\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics - once again, pay attention to AUC!\n",
    "print('Validation Labels:')\n",
    "print(labels_val)\n",
    "\n",
    "print('Predicted Labels:')\n",
    "y_pred = trained_model_aug.predict_classes(images_val, verbose=1).T[0]\n",
    "print(y_pred)\n",
    "\n",
    "print('Predicted Probabilities')\n",
    "y_probs = trained_model_aug.predict(images_val, verbose=1).T[0]\n",
    "print(y_probs)\n",
    "\n",
    "print('Prediction AUC')\n",
    "print(roc_auc_score(labels_val, y_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Playground.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
